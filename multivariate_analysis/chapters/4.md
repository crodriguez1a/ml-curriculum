# Data screening and transformations

This chapter discusses applying transformations to obtain more normal distributions.

## 4.2 Common transformations

To examine common transformations, we can examine a plot of X against its transformed self. For example, a plot of logX versus X.

**Logarithm (log) to the base 10**

> Recall that logarithm is number that satisfies the relationship X=10<sup>y</sup>. That is, the logarithm of X is the power Y to which 10 must be raised to produce X.

If X can have zero or negative values, an appropriate constant may be applied.

Remember that Logarithms can be taken to any base. When taken to the familiar base `e=2.7183`, they can be called **natural logarithms**.

> The natural logarithm is used frequently in theoretical studies because of certain appealing mathematical properties.

**Power transformations (p value)**

> ...the transformation X<sup>2</sup> (X raised to the power of 2) is used frequently in statistical formulas such as computing variance

The most commonly used are X raised to the power of ½ and the inverse X raised to the power of -1

Transformations are characterized by noting the power to which X is raised using the symbol `p`.

|Square Root| Inverse | Logarithmic |
|---|---|---
p = ½ | p = -1 | p = 0

The effects of these transformations in reducing long-tailed distribution to the right are greater as the value `p` decreases from 1.

**Exponential function**

> An exponential function of X may thought of as the antilogarithm of X

The function 10<sup>x</sup> has the same shape but increases faster than e<sup>x</sup>. See figure 4.3

## 4.3 Selecting appropriate transformations

The most common distribution for continuous observation is typically referred to as **Gaussian** (or normal) distribution.

**Assessing normality using histograms**

Most often, a histogram with normal distribution will resemble a bell-shaped Gaussian curve. The right side of the mean will mirror the left. See 4.4a

**Assessing normality using box plots**

Symmetry of the empirical distribution can be assessed by examining quantiles. Quantiles are obtained by first order N observations from smallest to largest. The ordered observations are used to compute the quantile using the formula:

```
Q(i) = (i - 0.5)/N
```

Percentiles are similar but are  computed dividing the sample into 100 equal parts.

Widely used quantiles include the sample median Q(0.5). Theoretically, for the normal distributions, the median equals the mean or are not very different. **Quartiles** Q(0.25) and Q(0.75) are also widely used.

> If the distribution is symmetric, then the difference between the median and Q(0.25) would equal the difference between Q(0.75) and the median. This is displayed graphically in **box plots**.

Q(0.75) and Q(0.25) are plotted at the top and bottom of the box and the median is denoted by either a line or dot within the box. The Lines is this case are called **whiskers** and extend from the ends of the box out the what are called **adjacent values**. Values that are are beyond the adjacent value are sometimes defined as outliers. Visual symmetry (when looking at distances from the median) most often denotes normal distribution.

**Assesing normality using normal probability or normal quantile plots**

> Normal probability plots present and appealing option for checking for normality.

This graph is a plot of the **cumulative distribution** found in the data set. If the data were from a normal distribution, the normal probability plot should approximate a straight line. See 4.4a. Although, even when the data are normally distributed, the plot may not be perfectly straight when a sample size is small. In this case, the middle 80 or 90% of the graph should approximate a straight line.

One example where a transformation can be applied is visible in figure 4.4c. Distribution is skewed to the right (longer tail on the right side). In this case we can create a new variable by taking the logarithm of X to base 10 or *e*.

Quantile-quantile plots called **normal quantile plots** represent theoretical (normal) distribution against the empirical quantiles of the data.

**Selecting a transformation to induce normality**

As noted, transformed data can sometimes approximate normal distribution. In some cases transformation is known. For example, the square root transformation is used with Poisson-distributed variables. The logarithmic transformation is often used when variables present long tails to the right.

Common transformations included in [Fisher and van Belle (1993)](https://books.google.com/books?hl=en&lr=&id=KSh8IOrLPzwC&oi=fnd&pg=PR7&ots=K5afJEhYII&sig=6Dv50xjemqIdd6e0hlr-1cO11-g#v=onepage&q=common%20transformations&f=false).

![workflow](img/common_transformations_fisher_van_belle_93.png "Common Transformations")

Another strategy for choosing a transformation is to progress up or down the values of `p` depending on the shape of the normal probability plot.

> If the plot looks like Figure 4.2c, then a value of `p` less than 1 is tried. Suppose `p = ½` is tried. If this is not sufficient and the normal probability plot is still curved downward at the ends, then `p = 0`, i.e., the logarithmic transformation, can be tried. If the plot appears to be a little too much, a positive constant can be added. Thus various transformations of the form `(X + C)`<sup>`p`</sup> are tried until the normal probability plot is as straight as possible.
