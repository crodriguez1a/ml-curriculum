# Probability and Information Theory

## Introduction

Probability Theory is a mathematical framework for representing uncertainty.

> It provides a means of quantifying uncertainty as well as axioms for deriving new uncertain statements.

In AI applications, probability theory is used in two major ways. First, the laws of probability tell us how systems should reason.

Second, we use probability and statistics to analyze behavior of proposed AI systems.

> While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution.

## 3.1 Why Probability?

Machine learning will always deal with uncertain quantities and sometimes stochastic (nondeterministic) quantities.

There are three possible sources of uncertainty:

1. Inherent stochasticity
1. Incomplete observability
1. Incomplete modeling

Probability related directly to the rates at which events occur is known as **frequentist probability**. Probability related to quantitative levels of certainty is know as **Bayesian probability**.

> Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are true or false given the assumption that some other set propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions.

## 3.2 Random Variables

A **random variable** is a variable that can take on different values randomly. Random variables may be discrete or continuous.

## 3.3 Probability Distributions

A **probability distribution** is a description of how likely a random variable is to take on each of its possible states.

A probability distribution over discrete variables may be described using a **probability mass function** (PMF).

> The probability mass function maps from a state of a random variable to the probability of that random variable taking on that state.

Probability mass functions can act on many variables simultaneously. A probability distribution over many variables is know as a **join probability distribution**.

When working with continuous random variables, we can describe probability distributions using a **probability density function** (PDF) rather than a PMF.

## 3.4 Marginal Probability

**Marginal probability** is the probability distribution over a subset of variables.

## 3.5 Conditional Probability

**Conditional probability** is the probability of some event given that some other event has occurred.

> It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change.

## 3.6 The Chain Rule of Conditional Probability

Any join probability distribution over many random variables can be decomposed into conditional distribution over one variable. This observation is known as the **chain** or **product rule**.

## 3.7 Independence and Conditional Independence

Random variables *x,y* are considered independent if their probability distribution can be expressed as a product of two factors, one involving only *x* and one involving only *y*.


## 3.8 Expectation, Variance and Covariance

The expected value (expectation) of some function *f(x)* with respect to a probability *P(x)* is the mean value that *f* takes on when *x* is drawn from *P*.

**Variance** gives a measure of how much the values of a function of a random variable *x* vary as we sample different values of *x* from its probability distribution.

The square root of the variance is known as the **standard deviation**.

**Covariance** gives some sense of how much two values are linearly related to one another.

> High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa.

Covariance is different from correlation which normalizes the contribution of each variable and only measures how much the variables are related.

> The notions of covariance and dependence are related, but are in fact distinct concepts. They are related because two variables that are independent have zero covariance, and two variables that have non-zero covariance are dependent. However, independence is a distinct property from covariance. For two variables to have zero covariance, there must be no linear dependence between them. Independence is a stronger requirement than zero covariance, because independence also excludes nonlinear relationships. It is possible for two variables to be dependent but have zero covariance.

## 3.9 Common Probability Distributions

The **Bernoulli distribution** is a distribution over a single binary variable.

The **multinoulli (categorical) distribution** is a distribution over a single discrete variable with *k* different states, where *k* is finite.

**Gaussian (normal) distribution** is the most commonly used distribution over real numbers.

> Normal distributions are a sensible choice for many applications. In the absence of prior knowledge about what form a distribution over the real numbers should take.

Normal distribution is a good default choice for two major reasons.

> First, many distributions we wish to model are truly close to being normal distributions. The **central limit theorem** shows that the sum of many independent random variables is approximately normally distributed.

> Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model.

In deep learning contexts, we often look to have a probability distribution with a sharp point at *x=0*. We can use **exponential distribution** to accomplish this.

> The exponential distribution uses the indicator function 1x≥0 to assign probability
zero to all negative values of x.

In some cases, we want to specify that all the mass in a probability distribution clusters around a single point. This can be accomplished by defining a PDF using the **Dirac delta function**

> The Dirac delta function is defined such that it is zero-valued everywhere except 0, yet integrates to 1. The Dirac delta function is not an ordinary function that associates each value x with a real-valued output, instead it is a different kind of  mathematical object called a generalized function that is defined in terms of its properties when integrated.

A common way of combining distributions is to assemble a **mixture distribution**. A mixture distribution is made up of several component distributions. The mixture model is a simple strategy for combining probability distributions to create a richer distribution.

> The mixture model allows us to briefly glimpse a concept that will be of paramount importance later - the **latent variable**.

## 3.10 Useful Properties of Common Functions

Certain probability functions are commonly used in deep learning models.

> The **logistic sigmoid** is commonly used to produce the theta parameter of a Bernoulli distribution because its range is (0,1), which lies within the valid range of values for the theta parameter. The sigmoid function **saturates** when its argument is very positive or very negative, meaning that the function becomes very flat and insensitive to small changes in its input.

Another commonly used function called the **softplus** function can be useful for producing the [β/beta](https://en.wikipedia.org/wiki/Beta) or [σ/sigma](https://en.wikipedia.org/wiki/Sigma) parameter of a normal distribution because its range is (0, ∞). It is also used when manipulating expressions involving sigmoids.

> The name of the softplus function comes from the fact that it is a smoothed or “softened” version of
x+ = max(0, x).

## 3.11 Bayes' Rule

> We often find ourselves in a situation where we know P(y | x) and need to know P (x | y). Fortunately, if we also know P (x), we can compute the desired quantity using Bayes’ rule:

P(x | y) =
P(x)P(y | x) / P (y)

## 3.13 Information theory

Information theory is a branch of applied math that involves quantifying how much information is present in a signal. It was originally invented to study sending messages over a noisy channel (e.g., radio transmission).

> In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes. In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply.

The basic intuition behind information theory is that an unlikely event can be more informative than an event likely to occur.

This book always uses *log* to represent the natural logarithm, with base *e* with units written in **nats**. One nat is the amount of information gained by observing an event of probability 1/e.

The amount of uncertainty in an entire probability distribution can quantified using the **Shannon entropy**.

The Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution.

> Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy.

When *x* is continuous, the Shannon entropy is referred to as the **differential entropy**.

The Kullback-Leibler (KL) divergence is used to  measure how different two separate probability distributions are over the same random variable *x*.

> The KL divergence has many useful properties, most notably that it is non- negative. The KL divergence is 0 if and only if P and Q are the same distribution in the case of discrete variables, or equal “almost everywhere” in the case of continuous variables.

Since KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring a distance between distributions.

**Cross-entropy** is a quantity closely related to the KL divergence. Minimizing the cross-entropy with respect to *Q* is equivalent to minimizing the KL divergence.

## 3.14 Structured Probabilistic Models

Machine learning algorithms can often involve probability distributions over a large number of random variables. Often, these distributions involve direct interactions between relatively few variables.

A probability distribution can be split into many factors that we multiply together.

> For example, suppose we have three random variables: a, b and c. Suppose that a influences the value of b and b influences the value of c, but that a and c are independent given b. We can represent the probability distribution over all three variables as a product of probability distributions over two variables

These factorizations greatly reduce the number of parameters used to describe the distribution.

This factorization can be represented with a graph called a **structured probabilistic model**, or a **graphical model**.

**Directed** models use graphs with directed edges to represent factorizations into conditional distributions.

**Undirected** models use graphs with undirected edges to present factorizations into a set of functions.

> unlike in the directed case, these functions are usually not probability distributions of any kind.
