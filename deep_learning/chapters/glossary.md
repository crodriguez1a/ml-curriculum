# Linear Algebra

- **Scalars**: A scalar is just a single number, in contrast to most other objects studied in linear algebra, which are usually arrays of multiple numbers.

- **Vectors**: A vector is an array of numbers. The numbers are arranged in order.

- **Matrices**: A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one.

- **Tensors**: In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.

- **Transpose**: A matrix operation that outputs the mirror image of the matrix across a diagonal line, called the **main diagonal**.

- **Identity Matrix**: An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix.

- **Linear Combination**: Formally, a
linear combination of some set of vectors is given by multiplying each vector by a corresponding scalar coefficient and adding the results

- **Norms**: are functions mapping vectors to non-negative
values

- **Euclidean Norm**: The L
2 norm, with p = 2, is known as the Euclidean norm. It is simply the
Euclidean distance from the origin to the point identified by x.

- **Diagonal Matrix**: Diagonal matrices consist mostly of zeros and have non-zero entries only along the main diagonal.  

- **Symmetric Matrix**: A symmetric matrix is any matrix that is equal to its own transpose.

- **Orthogonal Matrix**: An orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal

- **Eigenvector**: An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of *v*.

- **Eigenvalue**: The scalar Î» is known as the eigenvalue corresponding to this eigenvector.

- **Eigendecomposition**: One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and eigenvalues.
