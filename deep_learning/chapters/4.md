# Numerical Computation

Machine learning algorithms often require a lot of numerical computation. Algorithms solve mathematical problems by methods that iteratively update estimates of the solution. Common optimizations include finding the value of an argument that minimizes or maximizes a function.

> Even just evaluating a mathematical function on a digital computer can be difficult when the function involves real numbers, which cannot be represented precisely using a finite amount of memory.

## 4.1 Overflow and Underflow

The fundamental difficulty of computing continuous math is the need to represent many real numbers infinitely with a finite number of [bit patterns](https://encyclopedia2.thefreedictionary.com/bit+pattern). Naturally, we incur some approximation error when representing numbers in a computer. For example, compounding errors can occur when an algorithm is not designed to minimize the accumulation of rounding error.

One problematic example is **underflow** which occurs when numbers near zero are rounded to zero. This should be avoided for various reasons. For example, many computational environments will raise an error when attempting division by zero.

Another damaging form of error is **overflow**. Overflow occurs when numbers with large magnitude are approximated to infinity or negative infinity.

One function that should be stabilized against underflow and overflow is the **softmax function**. Softmax is often used to predict probabilities associated with multinoulli distribution.

Difficulties with overflow and underflow can be resolved by adding or subtracting a scalar from the input vector.

> Subtracting max-sub-i x-sub-i results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero.

The problem of underflow in the numerator can be solved similarly implementing a function that calculates log softmax in a numerically stable way.

See [Theano](http://deeplearning.net/software/theano/) provides examples of auto-detection and stabilization of commonly unstable expressions.

## 4.2 Poor Conditioning

## 4.3 Gradient-Based Optimization

## 4.4 Constrained Optimization

## 4.5 Example: Linear Least Squares
