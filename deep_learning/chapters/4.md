# Numerical Computation

Machine learning algorithms often require a lot of numerical computation. Algorithms solve mathematical problems by methods that iteratively update estimates of the solution. Common optimizations include finding the value of an argument that minimizes or maximizes a function.

> Even just evaluating a mathematical function on a digital computer can be difficult when the function involves real numbers, which cannot be represented precisely using a finite amount of memory.

## 4.1 Overflow and Underflow

The fundamental difficulty of computing continuous math is the need to represent many real numbers infinitely with a finite number of [bit patterns](https://encyclopedia2.thefreedictionary.com/bit+pattern). Naturally, we incur some approximation error when representing numbers in a computer. For example, compounding errors can occur when an algorithm is not designed to minimize the accumulation of rounding error.

One problematic example is **underflow** which occurs when numbers near zero are rounded to zero. This should be avoided for various reasons. For example, many computational environments will raise an error when attempting division by zero.

Another damaging form of error is **overflow**. Overflow occurs when numbers with large magnitude are approximated to infinity or negative infinity.

One function that should be stabilized against underflow and overflow is the **softmax function**. Softmax is often used to predict probabilities associated with multinoulli distribution.

Difficulties with overflow and underflow can be resolved by adding or subtracting a scalar from the input vector.

> Subtracting max-sub-i x-sub-i results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero.

The problem of underflow in the numerator can be solved similarly implementing a function that calculates log softmax in a numerically stable way.

See [Theano](http://deeplearning.net/software/theano/) provides examples of auto-detection and stabilization of commonly unstable expressions.

## 4.2 Poor Conditioning

Conditioning is a reference to how rapidly a function changes when there are small changes to its input. Functions that are sensitive to small changes in input can be problematic when rounding errors in the input result in large changes in output. Matrix inversion is particularly sensitive to error in the input.

> This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse. In practice, the error will be compounded further by numerical errors in the inversion process itself.

## 4.3 Gradient-Based Optimization

Most deep learning algorithms will require optimization of some kind.

> Optimization refers to the task of either minimizing or maximizing some function f(x) by altering x. We usually phrase most optimization problems in terms of minimizing f(x). Maximization may be accomplished via a minimization algorithm by minimizing âˆ’f (x).

A brief review of how calculus concepts relate to optimization:

> Suppose we have a function y = f(x), where both x and y are real numbers.
The **derivative** of this function is denoted as f(x) or as dx. The derivative f(x) gives the slope of f(x) at the point x. In other words, it specifies how to scale a small change in the input in order to obtain the corresponding change in the output

The derivative is useful for minimization because it tells us how to change x in order to make a small improvement in y.

> We can thus reduce f (x) by moving x in small steps with opposite sign of the derivative. This technique is called **gradient descent** (Cauchy, 1847)

When the derivative provides no information about which direction to move. Points where f(x) = 0 are known as **critical points** or **stationary points**.

> A **local minimum** is a point where f (x) is lower than at all neighboring points, so it is no longer possible to decrease f(x) by making infinitesimal steps.
A **local maximum** is a point where f (x) is higher than at all neighboring points, so it is not possible to increase f(x) by making infinitesimal steps.

![workflow](img/min_max_2.png "Mimimun, Maximum")

Some critical points are neither maxima or minima and are known as **saddle points**.

> A point that obtains the absolute lowest value of f(x) is a global minimum. It is possible for there to be only one global minimum or multiple global minima of the function. It is also possible for there to be local minima that are not globally optimal. In the context of deep learning, we optimize functions that may have many local minima that are not optimal, and many saddle points surrounded by very flat regions.

Functions that have multiple inputs are often minimized. For the concept of "minimization" to hold, there must still only be a single scalar output.

Functions with multiple inputs require applying the concept of **partial derivatives**.

> The partial derivative measures how f changes as only the x sub i
variable x sub i increases at point x.

The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector and contains all of the partial derivatives.

In higher dimensions, critical points are points where every element of the gradient is equal to zero.

> The **directional derivative** in direction u (a unit vector) is the slope of the
function f in direction u.

To minimize f, we would like to find the direction in which f decreases the fastest. This can be done using the directional derivative.

A function is minimized when the direction u points in the opposite direction of the gradient.

> In other words, the gradient points directly uphill, and the negative gradient points directly downhill. We can decrease f by moving in the direction of the negative gradient.
This is known as the method of steepest descent or gradient descent.

Gradient descent proposes a new point where (epsilon) is the **learning rate**, a positive scalar denoting the size of the step. Epsilon can be chosen with various approaches.

> A popular approach is to set epsilon to a small constant. Sometimes, we can solve for the step size that makes the directional derivative vanish.

Another approach is to evaluate f for several values of epsilon and choose the one that results in the smallest objective function value. This strategy is called **line search**.

Gradient descent converges when every element of the gradient is zero or very close to zero.

Gradient descent is limited to optimization in continuous spaces, but the general approach of making small incremental moves toward better configurations can generalized to discrete spaces.

> Ascending an objective function of discrete parameters is called hill climbing (Russel and Norvig, 2003)

At times we need to find all of partial derivatives of a function whose input and ouput are both vectors. The matrix containing all partial derivatives known as a **Jacobian Matrix**.

Sometimes we're interested in the derivative of a derivative or the **second derivative**.

> The **second derivative** tells us how the first derivative will change as we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring **curvature**.



## 4.4 Constrained Optimization

## 4.5 Example: Linear Least Squares
